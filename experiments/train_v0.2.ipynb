{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93057,"databundleVersionId":11145869,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10935216,"sourceType":"datasetVersion","datasetId":6800034},{"sourceId":10935226,"sourceType":"datasetVersion","datasetId":6800042},{"sourceId":10935233,"sourceType":"datasetVersion","datasetId":6800047}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torchvision\nimport torchvision.transforms as transforms\nimport pandas as pd\nimport os\nimport pickle\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:09:45.151626Z","iopub.execute_input":"2025-03-06T16:09:45.151872Z","iopub.status.idle":"2025-03-06T16:09:51.136233Z","shell.execute_reply.started":"2025-03-06T16:09:45.151848Z","shell.execute_reply":"2025-03-06T16:09:51.135534Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Resnet Architecture Setup","metadata":{}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:09:57.076037Z","iopub.execute_input":"2025-03-06T16:09:57.076357Z","iopub.status.idle":"2025-03-06T16:09:57.086229Z","shell.execute_reply.started":"2025-03-06T16:09:57.076320Z","shell.execute_reply":"2025-03-06T16:09:57.085229Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:09:58.513534Z","iopub.execute_input":"2025-03-06T16:09:58.513862Z","iopub.status.idle":"2025-03-06T16:09:58.521028Z","shell.execute_reply.started":"2025-03-06T16:09:58.513838Z","shell.execute_reply":"2025-03-06T16:09:58.520109Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# One of our first target models, otherwise known as ResNet10.\n# We use 1 block \ndef ResNet_v1():\n    return ResNet(BasicBlock, [1, 1, 1, 1])\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3, 4, 6, 3])\n\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3, 4, 6, 3])\n\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3, 4, 23, 3])\n\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3, 8, 36, 3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:09:59.714815Z","iopub.execute_input":"2025-03-06T16:09:59.715217Z","iopub.status.idle":"2025-03-06T16:09:59.722347Z","shell.execute_reply.started":"2025-03-06T16:09:59.715187Z","shell.execute_reply":"2025-03-06T16:09:59.721196Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Batching trainset and testsets","metadata":{}},{"cell_type":"code","source":"# Function to load one CIFAR batch (pickle file)\ndef load_cifar_batch(file):\n    with open(file, 'rb') as fo:\n        dict_data = pickle.load(fo, encoding='bytes')\n    return dict_data\n\n# Define the directory where the CIFAR-10 batch files are stored\ncifar10_dir = '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py'\n\n# Load label names from batches.meta\nmeta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\nlabel_names = meta_data_dict[b'label_names']  # labels are stored as bytes\n\n# Load and combine all training batches (data_batch_1 to data_batch_5)\ntrain_data_list = []\ntrain_labels = []\nfor i in range(1, 6):\n    batch_file = os.path.join(cifar10_dir, f'data_batch_{i}')\n    batch_dict = load_cifar_batch(batch_file)\n    # Reshape data from (10000, 3072) to (10000, 32, 32, 3)\n    batch_data = batch_dict[b'data']\n    batch_data = batch_data.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n    train_data_list.append(batch_data)\n    train_labels += batch_dict[b'labels']\n\n# Concatenate training batches to create the full training set\ntrain_images = np.concatenate(train_data_list, axis=0)\nprint(\"Training set:\", train_images.shape)\n\n# Load the test batch\ntest_batch = load_cifar_batch(os.path.join(cifar10_dir, 'test_batch'))\ntest_images = test_batch[b'data']\ntest_images = test_images.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\ntest_labels = test_batch[b'labels']\nprint(\"Test set:\", test_images.shape)\n\n# Define a custom dataset that applies transforms\nclass CIFAR10CustomDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images  # Expected shape: (N, 32, 32, 3)\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        # Convert the numpy image to a PIL image for transformation\n        img = Image.fromarray(self.images[index])\n        label = self.labels[index]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# Define transforms for training and testing (same as before)\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), \n                         (0.2023, 0.1994, 0.2010)),\n])\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), \n                         (0.2023, 0.1994, 0.2010)),\n])\n\n# Create dataset objects\ntrain_dataset = CIFAR10CustomDataset(train_images, train_labels, transform=transform_train)\ntest_dataset = CIFAR10CustomDataset(test_images, test_labels, transform=transform_test)\n\n# Create DataLoaders\ntrainloader = DataLoader(train_dataset, batch_size=2048, shuffle=True, num_workers=2)\ntestloader = DataLoader(test_dataset, batch_size=750, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:03.072217Z","iopub.execute_input":"2025-03-06T16:10:03.072496Z","iopub.status.idle":"2025-03-06T16:10:04.513987Z","shell.execute_reply.started":"2025-03-06T16:10:03.072475Z","shell.execute_reply":"2025-03-06T16:10:04.513037Z"}},"outputs":[{"name":"stdout","text":"Training set: (50000, 32, 32, 3)\nTest set: (10000, 32, 32, 3)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Model Assumptions","metadata":{}},{"cell_type":"code","source":"# Parameters (instead of argparse)\nlr = 0.001            # learning rate\nresume = True      # whether to resume from checkpoint\nnum_new_epochs = 100  # number of new epochs to run\nstart_epoch = 0     # start from epoch 0 or last checkpoint epoch\n\n# Define file paths in /kaggle/working\ncsv_file = '/kaggle/working/history/resnet1_metrics.csv'\ncurrent_checkpoint_file = '/kaggle/input/checkpoint/ckpt_resnetv1_e200.pth'\ncheckpoint_dir = '/kaggle/working/checkpoint'\n\n\n# Set device and beggining best acc\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0        # best test accuracy\n\n# Create directories if they don't exist\nos.makedirs(os.path.dirname(csv_file), exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)\ncheckpoint_file = os.path.join(checkpoint_dir, 'ckpt.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:08.381928Z","iopub.execute_input":"2025-03-06T16:10:08.382221Z","iopub.status.idle":"2025-03-06T16:10:08.431116Z","shell.execute_reply.started":"2025-03-06T16:10:08.382200Z","shell.execute_reply":"2025-03-06T16:10:08.430234Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Define Model","metadata":{}},{"cell_type":"code","source":"print('==> Building model..')\nnet = ResNet_v1()  # Use the ResNet18 model from your models module\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\n# Optionally resume from checkpoint\nif resume:\n    print('==> Resuming from checkpoint..')\n    if os.path.isfile(current_checkpoint_file):\n        checkpoint = torch.load(current_checkpoint_file)\n        net.load_state_dict(checkpoint['net'])\n        best_acc = checkpoint['acc']\n        start_epoch = checkpoint['epoch']\n    else:\n        print(\"No checkpoint file found. Starting from scratch.\")\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:13.561049Z","iopub.execute_input":"2025-03-06T16:10:13.561446Z","iopub.status.idle":"2025-03-06T16:10:13.985620Z","shell.execute_reply.started":"2025-03-06T16:10:13.561411Z","shell.execute_reply":"2025-03-06T16:10:13.984600Z"}},"outputs":[{"name":"stdout","text":"==> Building model..\n==> Resuming from checkpoint..\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-e350f7b39926>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(current_checkpoint_file)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Training Function","metadata":{}},{"cell_type":"code","source":"# Define training function\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        #print('Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    print('Total Loss: %.3f | Total Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n        \n    \n    train_avg_loss = train_loss / len(trainloader)\n    train_accuracy = 100. * correct / total\n    return train_avg_loss, train_accuracy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:18.119595Z","iopub.execute_input":"2025-03-06T16:10:18.119950Z","iopub.status.idle":"2025-03-06T16:10:18.126053Z","shell.execute_reply.started":"2025-03-06T16:10:18.119922Z","shell.execute_reply":"2025-03-06T16:10:18.124980Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Testset Function","metadata":{}},{"cell_type":"code","source":"# Define test function\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            \n            #print('Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n        print('Total Loss: %.3f | Total Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n    \n    test_avg_loss = test_loss / len(testloader)\n    test_accuracy = 100. * correct / total\n\n    # Save checkpoint if accuracy improves\n    acc = 100. * correct / total\n    if acc > best_acc:\n        print('Saving checkpoint..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        torch.save(state, checkpoint_file)\n        best_acc = acc\n\n    return test_avg_loss, test_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:20.512919Z","iopub.execute_input":"2025-03-06T16:10:20.513234Z","iopub.status.idle":"2025-03-06T16:10:20.521165Z","shell.execute_reply.started":"2025-03-06T16:10:20.513210Z","shell.execute_reply":"2025-03-06T16:10:20.520002Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if os.path.isfile(csv_file):\n    df_existing = pd.read_csv(csv_file)\n    last_epoch = df_existing[\"epoch\"].iloc[-1]\n    print(f\"Found existing CSV. Last recorded epoch: {last_epoch}\")\n    current_epoch = last_epoch + 1\nelse:\n    print(f\"CSV file not found. Starting at epoch: {start_epoch}\")\n    current_epoch = start_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:23.347887Z","iopub.execute_input":"2025-03-06T16:10:23.348194Z","iopub.status.idle":"2025-03-06T16:10:23.353975Z","shell.execute_reply.started":"2025-03-06T16:10:23.348173Z","shell.execute_reply":"2025-03-06T16:10:23.353025Z"}},"outputs":[{"name":"stdout","text":"CSV file not found. Starting at epoch: 199\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for epoch in range(current_epoch, current_epoch + num_new_epochs):\n    train_avg_loss, train_accuracy = train(epoch)\n    test_avg_loss, test_accuracy = test(epoch)\n    scheduler.step()\n\n    # Record metrics for the epoch.\n    df_epoch = pd.DataFrame([{\n        \"epoch\": epoch,\n        \"train_avg_loss\": train_avg_loss,\n        \"train_accuracy\": train_accuracy,\n        \"test_avg_loss\": test_avg_loss,\n        \"test_accuracy\": test_accuracy\n    }])\n    \n    # Append to CSV history.\n    if os.path.isfile(csv_file):\n        df_epoch.to_csv(csv_file, mode='a', header=False, index=False)\n    else:\n        df_epoch.to_csv(csv_file, mode='w', header=True, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:10:48.609667Z","iopub.execute_input":"2025-03-06T16:10:48.610031Z","iopub.status.idle":"2025-03-06T16:35:45.518692Z","shell.execute_reply.started":"2025-03-06T16:10:48.610004Z","shell.execute_reply":"2025-03-06T16:35:45.517696Z"}},"outputs":[{"name":"stdout","text":"\nEpoch: 199\nTotal Loss: 0.003 | Total Acc: 99.986% (49993/50000)\nTotal Loss: 0.281 | Total Acc: 92.660% (9266/10000)\nSaving checkpoint..\n\nEpoch: 200\nTotal Loss: 0.003 | Total Acc: 99.990% (49995/50000)\nTotal Loss: 0.279 | Total Acc: 92.710% (9271/10000)\nSaving checkpoint..\n\nEpoch: 201\nTotal Loss: 0.003 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.278 | Total Acc: 92.790% (9279/10000)\nSaving checkpoint..\n\nEpoch: 202\nTotal Loss: 0.003 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.276 | Total Acc: 92.850% (9285/10000)\nSaving checkpoint..\n\nEpoch: 203\nTotal Loss: 0.003 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.276 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 204\nTotal Loss: 0.003 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.274 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 205\nTotal Loss: 0.003 | Total Acc: 99.992% (49996/50000)\nTotal Loss: 0.274 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 206\nTotal Loss: 0.003 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.274 | Total Acc: 92.800% (9280/10000)\n\nEpoch: 207\nTotal Loss: 0.003 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.273 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 208\nTotal Loss: 0.003 | Total Acc: 99.992% (49996/50000)\nTotal Loss: 0.273 | Total Acc: 92.860% (9286/10000)\nSaving checkpoint..\n\nEpoch: 209\nTotal Loss: 0.003 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.272 | Total Acc: 92.820% (9282/10000)\n\nEpoch: 210\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.272 | Total Acc: 92.840% (9284/10000)\n\nEpoch: 211\nTotal Loss: 0.003 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.272 | Total Acc: 92.840% (9284/10000)\n\nEpoch: 212\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.271 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 213\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.271 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 214\nTotal Loss: 0.003 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.271 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 215\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.271 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 216\nTotal Loss: 0.002 | Total Acc: 99.990% (49995/50000)\nTotal Loss: 0.271 | Total Acc: 92.760% (9276/10000)\n\nEpoch: 217\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.271 | Total Acc: 92.760% (9276/10000)\n\nEpoch: 218\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.270 | Total Acc: 92.800% (9280/10000)\n\nEpoch: 219\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.270 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 220\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.270 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 221\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.270 | Total Acc: 92.800% (9280/10000)\n\nEpoch: 222\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.270 | Total Acc: 92.810% (9281/10000)\n\nEpoch: 223\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.270 | Total Acc: 92.810% (9281/10000)\n\nEpoch: 224\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.271 | Total Acc: 92.820% (9282/10000)\n\nEpoch: 225\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.270 | Total Acc: 92.810% (9281/10000)\n\nEpoch: 226\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.269 | Total Acc: 92.810% (9281/10000)\n\nEpoch: 227\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.269 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 228\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.790% (9279/10000)\n\nEpoch: 229\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.760% (9276/10000)\n\nEpoch: 230\nTotal Loss: 0.002 | Total Acc: 99.990% (49995/50000)\nTotal Loss: 0.269 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 231\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.890% (9289/10000)\nSaving checkpoint..\n\nEpoch: 232\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 233\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.820% (9282/10000)\n\nEpoch: 234\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 235\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 236\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 237\nTotal Loss: 0.002 | Total Acc: 99.992% (49996/50000)\nTotal Loss: 0.269 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 238\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 239\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.268 | Total Acc: 92.840% (9284/10000)\n\nEpoch: 240\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.840% (9284/10000)\n\nEpoch: 241\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.830% (9283/10000)\n\nEpoch: 242\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.840% (9284/10000)\n\nEpoch: 243\nTotal Loss: 0.002 | Total Acc: 99.990% (49995/50000)\nTotal Loss: 0.268 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 244\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 245\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 246\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 247\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.810% (9281/10000)\n\nEpoch: 248\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.269 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 249\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.910% (9291/10000)\nSaving checkpoint..\n\nEpoch: 250\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 251\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.900% (9290/10000)\n\nEpoch: 252\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.268 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 253\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 254\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 255\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 256\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 257\nTotal Loss: 0.002 | Total Acc: 99.992% (49996/50000)\nTotal Loss: 0.268 | Total Acc: 92.930% (9293/10000)\nSaving checkpoint..\n\nEpoch: 258\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 259\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.268 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 260\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.950% (9295/10000)\nSaving checkpoint..\n\nEpoch: 261\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 262\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 263\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.930% (9293/10000)\n\nEpoch: 264\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.268 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 265\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 266\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.268 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 267\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.267 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 268\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 269\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 270\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.267 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 271\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 272\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.267 | Total Acc: 92.900% (9290/10000)\n\nEpoch: 273\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.920% (9292/10000)\n\nEpoch: 274\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 275\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 276\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.940% (9294/10000)\n\nEpoch: 277\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 278\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.266 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 279\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.267 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 280\nTotal Loss: 0.002 | Total Acc: 99.994% (49997/50000)\nTotal Loss: 0.267 | Total Acc: 92.900% (9290/10000)\n\nEpoch: 281\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 282\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 283\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 284\nTotal Loss: 0.002 | Total Acc: 99.996% (49998/50000)\nTotal Loss: 0.266 | Total Acc: 92.900% (9290/10000)\n\nEpoch: 285\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.900% (9290/10000)\n\nEpoch: 286\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 287\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.910% (9291/10000)\n\nEpoch: 288\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.850% (9285/10000)\n\nEpoch: 289\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 290\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 291\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 292\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.267 | Total Acc: 92.880% (9288/10000)\n\nEpoch: 293\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.266 | Total Acc: 92.860% (9286/10000)\n\nEpoch: 294\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 295\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.266 | Total Acc: 92.890% (9289/10000)\n\nEpoch: 296\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.266 | Total Acc: 92.870% (9287/10000)\n\nEpoch: 297\nTotal Loss: 0.002 | Total Acc: 100.000% (50000/50000)\nTotal Loss: 0.267 | Total Acc: 92.920% (9292/10000)\n\nEpoch: 298\nTotal Loss: 0.002 | Total Acc: 99.998% (49999/50000)\nTotal Loss: 0.266 | Total Acc: 92.870% (9287/10000)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file path\nfile_path = \"/kaggle/working/history/resnet1_metrics.csv\"\n\n# Read the CSV file\ndf = pd.read_csv(file_path)\n\n# Print the contents of the dataframe\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:36:08.437126Z","iopub.execute_input":"2025-03-06T16:36:08.437426Z","iopub.status.idle":"2025-03-06T16:36:08.461047Z","shell.execute_reply.started":"2025-03-06T16:36:08.437402Z","shell.execute_reply":"2025-03-06T16:36:08.460131Z"}},"outputs":[{"name":"stdout","text":"    epoch  train_avg_loss  train_accuracy  test_avg_loss  test_accuracy\n0     199        0.003357          99.986       0.281433          92.66\n1     200        0.003081          99.990       0.278881          92.71\n2     201        0.003026          99.994       0.277577          92.79\n3     202        0.002877          99.996       0.276467          92.85\n4     203        0.002756          99.994       0.275557          92.83\n..    ...             ...             ...            ...            ...\n95    294        0.002028         100.000       0.266346          92.89\n96    295        0.001997          99.998       0.265762          92.89\n97    296        0.002044         100.000       0.266294          92.87\n98    297        0.002009         100.000       0.266666          92.92\n99    298        0.002063          99.998       0.265882          92.87\n\n[100 rows x 5 columns]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"checkpoint_file = '/kaggle/working/checkpoint/ckpt.pth'\nif os.path.isfile(checkpoint_file):\n    # Load the checkpoint (using map_location='cpu' to ensure it loads even if CUDA isn't available)\n    checkpoint = torch.load(checkpoint_file, map_location='cpu')\n    \n    # Print the keys and a summary of the checkpoint content\n    print(\"Checkpoint keys:\", list(checkpoint.keys()))\n    print(\"\\nFull checkpoint contents:\")\n    for key, value in checkpoint.items():\n        if key == 'net':\n            print(f\"{key}: state_dict with {len(value)} keys\")\n        else:\n            print(f\"{key}: {value}\")\nelse:\n    print(\"Checkpoint file not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:36:13.399101Z","iopub.execute_input":"2025-03-06T16:36:13.399410Z","iopub.status.idle":"2025-03-06T16:36:13.424892Z","shell.execute_reply.started":"2025-03-06T16:36:13.399381Z","shell.execute_reply":"2025-03-06T16:36:13.423897Z"}},"outputs":[{"name":"stdout","text":"Checkpoint keys: ['net', 'acc', 'epoch']\n\nFull checkpoint contents:\nnet: state_dict with 74 keys\nacc: 92.95\nepoch: 260\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-13-9ab58dfd9aeb>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_file, map_location='cpu')\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport pickle\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\n\n# Create predictions directory if it doesn't exist.\npredictions_dir = '/kaggle/working/predictions'\nos.makedirs(predictions_dir, exist_ok=True)\npredictions_filename = os.path.join(predictions_dir, 'resnetv1_e300_r003.csv')\n\n# Define paths for the test set and checkpoint.\ntestset_path = '/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl'\ncheckpoint_path = '/kaggle/working/checkpoint/ckpt.pth'\n\n# Set device for inference.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Function to load a CIFAR batch from a pickle file.\ndef load_cifar_batch(file):\n    with open(file, 'rb') as fo:\n        batch = pickle.load(fo, encoding='bytes')\n    return batch\n\n# Load the hidden test batch.\ncifar10_batch = load_cifar_batch(testset_path)\n\n# Extract images; the test data is in (N x W x H x C) format.\nimages = cifar10_batch[b'data']\n\n# Ensure images are uint8 if they are in a numpy array.\nif isinstance(images, np.ndarray) and images.dtype != 'uint8':\n    images = images.astype('uint8')\n\n# Define the test transform.\ntransform_test = transforms.Compose([\n    transforms.ToPILImage(),  # Convert numpy array (H x W x C) to a PIL Image.\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Create a custom Dataset for the test images.\nclass TestDataset(Dataset):\n    def __init__(self, images, transform=None):\n        self.images = images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, idx  # Return index as the image ID.\n\n# Create the test dataset and DataLoader.\ntest_dataset = TestDataset(images, transform=transform_test)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n\n# Load the best model checkpoint.\nassert os.path.isfile(checkpoint_path), \"Checkpoint file not found!\"\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nstate_dict = checkpoint['net']\n\nfrom collections import OrderedDict\n# Initialize the model and load the state.\nnet = ResNet_v1()\nmodel = net.to(device)   \nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    new_key = k.replace(\"module.\", \"\")  # remove \"module.\" prefix\n    new_state_dict[new_key] = v\n\nmodel.load_state_dict(new_state_dict)\nmodel.eval()\n\n# Generate predictions on the test set.\npredictions = []\nwith torch.no_grad():\n    for imgs, ids in test_loader:\n        imgs = imgs.to(device)\n        outputs = model(imgs)\n        _, preds = outputs.max(1)\n        preds = preds.cpu().numpy()\n        for image_id, pred in zip(ids.numpy(), preds):\n            predictions.append({'ID': image_id, 'Label': int(pred)})\n\n# Convert predictions to a DataFrame and save as CSV.\ndf_predictions = pd.DataFrame(predictions)\ndf_predictions.to_csv(predictions_filename, index=False)\nprint(f\"Predictions saved to {predictions_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T16:36:52.771929Z","iopub.execute_input":"2025-03-06T16:36:52.772236Z","iopub.status.idle":"2025-03-06T16:36:56.235658Z","shell.execute_reply.started":"2025-03-06T16:36:52.772213Z","shell.execute_reply":"2025-03-06T16:36:56.234774Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-0fd4206ef325>:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/predictions/resnetv1_e300_r003.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}